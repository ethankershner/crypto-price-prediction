{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mobile-abuse",
   "metadata": {},
   "source": [
    "# Predicting Cryptocurrency Price Change with Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-delaware",
   "metadata": {},
   "source": [
    "## 0.1 Background\n",
    "\n",
    "This work builds on numerous academic papers that have found that tweet sentiment and volume are able to succesfully predict changes in cryptocurrency price (Stenqvist & Lonno, 2017, Coliani, Rosales & Signorotti, 2015). This work is unique in that it focuses on making predictions during shorter intervals than are typically examined by the majority of academic papers on this topic. Additionally, instead of only performing prediction modeling for Bitcoin, tweets related to Ethereum and Ethereum market data were collected alongside data pertaining to Bitcoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-fleet",
   "metadata": {},
   "source": [
    "## 0.2 Intent\n",
    "\n",
    "In this notebook previously collected data consisting of tweets related to Bitcoin and Ethereum as well as Bitcoin and Ethereum market data (including price and volume) are first cleaned and prepared for further use. Machine learning models are then instantiated and trained to make predictions about the direction of price changes using this data. Model accuracy is assessed using repeated k-fold cross validation. A variety of time intervals (time periods over which data is aggregated in order to predict cryptocurrency prices at the end of the next interval) are tested and results compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-camel",
   "metadata": {},
   "source": [
    "## 0.3 Data Collection\n",
    "\n",
    "Data collection scripts are available in the 'data-gatherer' subdirectory of this repository.\n",
    "\n",
    "Data collection scripts were loaded to a dedicated server to continual data collecting. Using the [tweepy API](https://www.tweepy.org/) two real-time streams of tweets containing \"bitcoin\" (for the bitcoin stream) or \"ethereum\" (for the ethereum stream) were instantiated. Tweet data, consisting of a timestamp and tweet text were saved to respective CSV files. Tweet streams were closed and re-opened every 15 minutes to prevent potential errors from prematurely ending data collection.\n",
    "\n",
    "[Coinmarketcap API](https://coinmarketcap.com/api/) was accessed every 15 minutes, and Bitcoin and Ethereum price and 24hr volume collected and saved to respective CSV files.\n",
    "\n",
    "Using these methods data was collected for a period of 19 days between 2/18/21 and 3/09/21. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-celebration",
   "metadata": {},
   "source": [
    "## 0.4 Description of Data\n",
    "\n",
    "During the collection period 1,048,575 tweets containing 'bitcoin' were collected and 115,395 tweets containing 'ethereum'.\n",
    "\n",
    "**Tweet Data**:\n",
    "\n",
    "- Timestamp\n",
    "- Text (converted to sentiment score in this notebook)\n",
    "- Volume (calculated in this notebook)\n",
    "\n",
    "**Cryptocurrency Market Data**:\n",
    "\n",
    "- Timestamp\n",
    "- Current Price (averaged over numerous exchanges)\n",
    "- 24HR volume (averaged over numerous exchanges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-cutting",
   "metadata": {},
   "source": [
    "## 1. Loading and Cleaning Data\n",
    "\n",
    "The data is loaded from CSV files into Pandas dataframes. Whitespace and links are removed from the tweet text. Tweets containing words that were observed to occur frequently in \"spam tweets\" (advertisements that are tweeted on a frequent basis) are removed from the dataset. Finally, duplicate tweets are removed due to being more likely to be an advertisement or a twitter bot providing numerical cryptocurrency information on a regular basis. A column containing the text without alphanumeric characters is used for the purpose of duplicate removal in order to remove tweets from accounts sharing numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defensive-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Token: 'BTC' for bitcoin, 'ETH' for ethereum\n",
    "def loadTweetData(token): \n",
    "\n",
    "    data = pd.read_csv('{}-tweets.csv'.format(token),parse_dates=['datetime'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shaped-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreives the market data. Required input: interval in 15 minutes increments i.e. 1 hour = 4\n",
    "\n",
    "def loadMarketData(token,sample):\n",
    "    \n",
    "    marketdata = pd.read_csv('{}-quotes.csv'.format(token),parse_dates=['datetime'],index_col = \"datetime\")\n",
    "    marketdata = marketdata.resample(sample).first()\n",
    "\n",
    "    return marketdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suspended-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanData(data):\n",
    "    \n",
    "    text = data.text.values\n",
    "    junk = np.zeros(len(text))\n",
    "    \n",
    "    filterwords = ['giveaway','latokens','want to experience','retweet']\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        # Removes whitespace\n",
    "        text[i] = text[i].strip()\n",
    "        text[i] = ' '.join(text[i].split())\n",
    "        # Removes ... from end of text if present\n",
    "        if(text[i] != '' and text[i][-1] == '…'):\n",
    "            text[i] = text[i][:-1]\n",
    "        # Removes links\n",
    "        text[i] = re.sub(r'http\\S+', '', text[i])\n",
    "        # Removes whitespace\n",
    "        text[i] = text[i].strip()\n",
    "        text[i] = ' '.join(text[i].split())\n",
    "        \n",
    "        for word in filterwords:\n",
    "            if word in text[i].lower():\n",
    "                junk[i] = 1.0\n",
    "        \n",
    "    # Replaces data text with whitespace-removed text    \n",
    "        \n",
    "    data['text'] = pd.DataFrame(text)\n",
    "        \n",
    "    # Adds \"junk\" vector to data\n",
    "    data['junk'] = pd.DataFrame(junk)\n",
    "        \n",
    "    # Removes tweets where junk = 1.0\n",
    "    data = data[data.junk == 0]\n",
    "        \n",
    "    data = data.drop('junk',axis=1)\n",
    "    \n",
    "    # Removes duplicates, first removing non-alphabetical characters\n",
    "    \n",
    "    text = data['text'].values\n",
    "    \n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        text[i] = (regex.sub(' ' ,text[i]))\n",
    "        text[i] = ' '.join(text[i].split())\n",
    "        text[i] = text[i].lower()\n",
    "        \n",
    "    data['alphatext'] = pd.DataFrame(text)\n",
    "    \n",
    "    data = data.drop_duplicates(subset='alphatext')\n",
    "    \n",
    "    # Reset index\n",
    "    \n",
    "    data = data.reset_index(drop=True)\n",
    "        \n",
    "    return data.drop(['alphatext'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-chemical",
   "metadata": {},
   "source": [
    "## 2. Add Sentiment Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-trust",
   "metadata": {},
   "source": [
    "Sentiment scores are calculated using the [Vader Sentiment Intensity Analyzer](https://github.com/cjhutto/vaderSentiment). Scores for each tweet are calculated and added as a column to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opening-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def addSentimentScores(data):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    text = data['text'].values\n",
    "    \n",
    "    sentiment = np.zeros(len(text))\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        compound = analyzer.polarity_scores(text[i])['compound']\n",
    "        sentiment[i] = compound\n",
    "        \n",
    "    data['sentiment'] = pd.DataFrame(sentiment)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-beginning",
   "metadata": {},
   "source": [
    "## 3. Get Mean Tweet Sentiment, Tweet Volume Change, and Price Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-uncertainty",
   "metadata": {},
   "source": [
    "Twitter data is then aggregated over the selected timesample period (15 minutes, 2 hours, etc.). Sentiment scores of the tweets within each time interval are averaged. Tweet volume for the interval is calculated. In addition, the price and volume change during each interval, calculated as the difference between the price and volume values for the next timestamp and the values for the current timestamp) are added as columns. An increase in the price/volume during the interval is coded as 1 and decrease or no change as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collective-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signFunction(x):\n",
    "    if(x)>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "applicable-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIntervalData(token,sample):\n",
    "    \n",
    "    data = loadTweetData(token)\n",
    "    data = cleanData(data)\n",
    "    data = addSentimentScores(data)\n",
    "    \n",
    "    prices = loadMarketData(token,sample)\n",
    "    \n",
    "    data = data.set_index('datetime')\n",
    "    data = data.resample(sample).agg({'sentiment':'mean','text':'count'})\n",
    "    data = data.join(prices)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    price_volume = data[['price','volume']].diff(periods=-1)\n",
    "    price_volume = -price_volume\n",
    "    \n",
    "    \n",
    "    price_volume['price'] = price_volume['price'].apply(signFunction)\n",
    "    price_volume['volume'] = price_volume['volume'].apply(signFunction)\n",
    "    \n",
    "    data['price'] = price_volume['price']\n",
    "    data['volume'] = price_volume['volume']\n",
    "    \n",
    "    data = data.dropna()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-importance",
   "metadata": {},
   "source": [
    "## 4. Predict Direction of Price Movement\n",
    "\n",
    "Using mean sentiment score, tweet volume, and trading volume change as features and change in price as the targer, Logistic Regression and 3 machine learning models are instantiated. Performance is evaluated using repeated k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hairy-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def predictPriceDirection(X,Y):\n",
    "\n",
    "    scores = {}\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3,random_state=1)\n",
    "    score = cross_val_score(model,X,Y,cv=cv,n_jobs=1)\n",
    "    score = sum(score)/len(score) \n",
    "    \n",
    "    scores['logistic_regression'] = score\n",
    "\n",
    "    model = BernoulliNB()\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3,random_state=1)\n",
    "    score = cross_val_score(model,X,Y,cv=cv,n_jobs=1)\n",
    "    score = sum(score)/len(score) \n",
    "\n",
    "    scores['bernoulli_nb'] = score\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3,random_state=1)\n",
    "    score = cross_val_score(model,X,Y,cv=cv,n_jobs=1)\n",
    "    score = sum(score)/len(score) \n",
    "\n",
    "    scores['random_forest'] = score\n",
    "\n",
    "    model = AdaBoostClassifier()\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3,random_state=1)\n",
    "    score = cross_val_score(model,X,Y,cv=cv,n_jobs=1)\n",
    "    score = sum(score)/len(score) \n",
    "\n",
    "    scores['adaboost'] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-resident",
   "metadata": {},
   "source": [
    "## 5. Test Different Time Intervals\n",
    "\n",
    "Model performance for 4 time intervals is collected and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cleared-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each of the specified intervals for each of the specified tokens\n",
    "\n",
    "# Tokens: list of tokens to test, Intervals: List of intervals to test\n",
    "\n",
    "def testIntervals(tokens,intervals):\n",
    "    \n",
    "    for t in tokens:\n",
    "        for i in intervals:\n",
    "            data = getIntervalData(t,i)\n",
    "            X = data[['text','sentiment','volume']]\n",
    "            Y = data['price'].values\n",
    "            scores = predictPriceDirection(X,Y)\n",
    "            \n",
    "            print('''{} --- {} --- Logistic Regression: {} --- Bernoulli NB: {} --- Random Forest: {} --- ADABoost: {}\n",
    "                    '''.format(t,i,scores['logistic_regression'],scores['bernoulli_nb'],scores['random_forest'],\n",
    "                        scores['adaboost']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "approved-breeding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC --- 15MIN --- Logistic Regression: 0.6662847363887756 --- Bernoulli NB: 0.6662847363887756 --- Random Forest: 0.5916700666394669 --- ADABoost: 0.6544029647762818\n",
      "                    \n",
      "BTC --- 30MIN --- Logistic Regression: 0.6134792521659824 --- Bernoulli NB: 0.6158093935248518 --- Random Forest: 0.5048791609667121 --- ADABoost: 0.5836251709986321\n",
      "                    \n",
      "BTC --- 1H --- Logistic Regression: 0.5612403100775192 --- Bernoulli NB: 0.5790697674418602 --- Random Forest: 0.5015503875968993 --- ADABoost: 0.5263565891472869\n",
      "                    \n",
      "BTC --- 2H --- Logistic Regression: 0.5463924963924963 --- Bernoulli NB: 0.5826118326118327 --- Random Forest: 0.5826118326118326 --- ADABoost: 0.5036075036075036\n",
      "                    \n",
      "ETH --- 15MIN --- Logistic Regression: 0.6576144816364482 --- Bernoulli NB: 0.6583118317061831 --- Random Forest: 0.5569979079497908 --- ADABoost: 0.6532932357043236\n",
      "                    \n",
      "ETH --- 30MIN --- Logistic Regression: 0.6300390266299358 --- Bernoulli NB: 0.6305945821854915 --- Random Forest: 0.5535123966942147 --- ADABoost: 0.606471533516988\n",
      "                    \n",
      "ETH --- 1H --- Logistic Regression: 0.5717577413479052 --- Bernoulli NB: 0.5789799635701275 --- Random Forest: 0.5274408014571949 --- ADABoost: 0.5246812386156647\n",
      "                    \n",
      "ETH --- 2H --- Logistic Regression: 0.5348387096774193 --- Bernoulli NB: 0.5480286738351253 --- Random Forest: 0.47720430107526884 --- ADABoost: 0.489247311827957\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "testIntervals(['BTC','ETH'],['15MIN','30MIN','1H','2H'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-military",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-novelty",
   "metadata": {},
   "source": [
    "For both Bitcoin and Ethereum, the models performed best when predicting at 15 minute intervals, with decreasing accuracy after adding time to the interval. Logistic regression performed better than the machine learning models and resulted in ~67% prediction accuracy for Bitcoin and ~66% prediction accuracy for Ethereum with 15 minute interval data.\n",
    "\n",
    "This may mean that price is more sensitive to short-term changes in tweet sentiment, tweet volume, and trade volume change, or alternatively that smaller time samples provided more training data and subsequently better models. These questions could be answered by more frequent collection of market data as well as by collecting data for a longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-management",
   "metadata": {},
   "source": [
    "## 7. Work Cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-toolbox",
   "metadata": {},
   "source": [
    "1. Stenqvist, Evita, and Jacob Lönnö. \"Predicting Bitcoin price fluctuation with Twitter sentiment analysis.\" (2017).\n",
    "2. Colianni, Stuart, Stephanie Rosales, and Michael Signorotti. \"Algorithmic trading of cryptocurrency based on Twitter sentiment analysis.\" CS229 Project (2015): 1-5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
